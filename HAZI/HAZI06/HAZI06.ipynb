{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1.  Értelmezd az adatokat!!!\\n    A feladat megoldásához használd a NJ transit + Amtrack csv-t a moodle-ból.\\n    A NJ-60k az a megoldott. Azt fogom használni a modellek teszteléséhez, illetve össze tudod hasonlítani az eredményedet.    \\n\\n2. Írj egy osztályt a következő feladatokra:  \\n     2.1 Neve legyen NJCleaner és mentsd el a NJCleaner.py-ba. Ebben a fájlban csak ez az osztály legyen.\\n     2.2 Konsturktorban kapja meg a csv elérési útvonalát és olvassa be pandas segítségével és mentsük el a data (self.data) osztályszintű változóba \\n     2.3 Írj egy függvényt ami sorbarendezi a dataframe-et 'scheduled_time' szerint növekvőbe és visszatér a sorbarendezett df-el, a függvény neve legyen 'order_by_scheduled_time' és térjen vissza a df-el  \\n     2.4 Dobjuk el a from és a to oszlopokat, illetve azokat a sorokat ahol van nan és adjuk vissza a df-et. A függvény neve legyen 'drop_columns_and_nan' és térjen vissza a df-el  \\n     2.5 A date-et alakítsd át napokra, pl.: 2018-03-01 --> Thursday, ennek az oszlopnak legyen neve a 'day'. Ezután dobd el a 'date' oszlopot és térjen vissza a df-el. A függvény neve legyen 'convert_date_to_day' és térjen vissza a df-el   \\n     2.6 Hozz létre egy új oszlopot 'part_of_the_day' névvel. A 'scheduled_time' oszlopból számítsd ki az alábbi értékeit. A 'scheduled_time'-ot dobd el. A függvény neve legyen 'convert_scheduled_time_to_part_of_the_day' és térjen vissza a df-el  \\n         4:00-7:59 -- early_morning  \\n         8:00-11:59 -- morning  \\n         12:00-15:59 -- afternoon  \\n         16:00-19:59 -- evening  \\n         20:00-23:59 -- night  \\n         0:00-3:59 -- late_night  \\n    2.7 A késéseket jelöld az alábbiak szerint. Az új osztlop neve legyen 'delay'. A függvény neve legyen pedig 'convert_delay' és térjen vissza a df-el\\n         0min <= x < 5min   --> 0  \\n         5min <= x          --> 1  \\n    2.8 Dobd el a felesleges oszlopokat 'train_id' 'actual_time' 'delay_minutes'. A függvény neve legyen 'drop_unnecessary_columns' és térjen vissza a df-el\\n    2.9 Írj egy olyan metódust, ami elmenti a dataframe első 60 000 sorát. A függvénynek egy string paramétere legyen, az pedig az, hogy hova mentse el a csv-t (pl.: 'data/NJ.csv'). A függvény neve legyen 'save_first_60k'. \\n    2.10 Írj egy függvényt ami a fenti függvényeket összefogja és megvalósítja (sorbarendezés --> drop_columns_and_nan --> ... --> save_first_60k), a függvény neve legyen 'prep_df'. Egy paramnétert várjon, az pedig a csv-nek a mentési útvonala legyen. Ha default value-ja legyen 'data/NJ.csv'\\n\\n3.  A feladatot a HAZI06.py-ban old meg.\\n    Az órán megírt DecisionTreeClassifier-t fit-eld fel az első feladatban lementett csv-re. \\n    A feladat célja az, hogy határozzuk meg azt, hogy a vonatok késnek-e vagy sem. 0p <= x < 5p --> nem késik (0), ha 5p <= x --> késik (1).\\n    Az adatoknak a 20% legyen test és a splitelés random_state-je pedig 41 (mint órán)\\n    A testset-en 80% kell elérni. Ha megvan a minimum százalék, akkor azzal paraméterezd fel a decisiontree-t és azt kell leadni.\\n\\n    A leadásnál csak egy fit kell, ezt azzal a paraméterre paraméterezd fel, amivel a legjobb accuracy-t elérted.\\n\\n    A helyes paraméter megtalálásához használhatsz grid_search-öt.\\n    https://www.w3schools.com/python/python_ml_grid_search.asp \\n\\n4.  A tanításodat foglald össze 4-5 mondatban a HAZI06.py-ban a fájl legalján kommentben. Írd le a nehézségeket, mivel próbálkoztál, mi vált be és mi nem. Ezen kívül írd le 10 fitelésed eredményét is, hogy milyen paraméterekkel probáltad és milyen accuracy-t értél el. \\nHa ezt feladatot hiányzik, akkor nem fogadjuk el a házit!\\n\\nHAZI-\\n    HAZI06-\\n        -NJCleaner.py\\n        -HAZI06.py\\n\\n##################################################################\\n##                                                              ##\\n## A feladatok közül csak a NJCleaner javítom unit test-el      ##\\n## A decision tree-t majd manuálisan fogom lefuttatni           ##\\n## NJCleaner - 10p, Tanítás - acc-nál 10%-ként egy pont         ##\\n## Ha a 4. feladat hiányzik, akkor nem tudjuk elfogadni a házit ##\\n##                                                              ##\\n##################################################################\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.  Értelmezd az adatokat!!!\n",
    "    A feladat megoldásához használd a NJ transit + Amtrack csv-t a moodle-ból.\n",
    "    A NJ-60k az a megoldott. Azt fogom használni a modellek teszteléséhez, illetve össze tudod hasonlítani az eredményedet.    \n",
    "\n",
    "2. Írj egy osztályt a következő feladatokra:  \n",
    "     2.1 Neve legyen NJCleaner és mentsd el a NJCleaner.py-ba. Ebben a fájlban csak ez az osztály legyen.\n",
    "     2.2 Konsturktorban kapja meg a csv elérési útvonalát és olvassa be pandas segítségével és mentsük el a data (self.data) osztályszintű változóba \n",
    "     2.3 Írj egy függvényt ami sorbarendezi a dataframe-et 'scheduled_time' szerint növekvőbe és visszatér a sorbarendezett df-el, a függvény neve legyen 'order_by_scheduled_time' és térjen vissza a df-el  \n",
    "     2.4 Dobjuk el a from és a to oszlopokat, illetve azokat a sorokat ahol van nan és adjuk vissza a df-et. A függvény neve legyen 'drop_columns_and_nan' és térjen vissza a df-el  \n",
    "     2.5 A date-et alakítsd át napokra, pl.: 2018-03-01 --> Thursday, ennek az oszlopnak legyen neve a 'day'. Ezután dobd el a 'date' oszlopot és térjen vissza a df-el. A függvény neve legyen 'convert_date_to_day' és térjen vissza a df-el   \n",
    "     2.6 Hozz létre egy új oszlopot 'part_of_the_day' névvel. A 'scheduled_time' oszlopból számítsd ki az alábbi értékeit. A 'scheduled_time'-ot dobd el. A függvény neve legyen 'convert_scheduled_time_to_part_of_the_day' és térjen vissza a df-el  \n",
    "         4:00-7:59 -- early_morning  \n",
    "         8:00-11:59 -- morning  \n",
    "         12:00-15:59 -- afternoon  \n",
    "         16:00-19:59 -- evening  \n",
    "         20:00-23:59 -- night  \n",
    "         0:00-3:59 -- late_night  \n",
    "    2.7 A késéseket jelöld az alábbiak szerint. Az új osztlop neve legyen 'delay'. A függvény neve legyen pedig 'convert_delay' és térjen vissza a df-el\n",
    "         0min <= x < 5min   --> 0  \n",
    "         5min <= x          --> 1  \n",
    "    2.8 Dobd el a felesleges oszlopokat 'train_id' 'actual_time' 'delay_minutes'. A függvény neve legyen 'drop_unnecessary_columns' és térjen vissza a df-el\n",
    "    2.9 Írj egy olyan metódust, ami elmenti a dataframe első 60 000 sorát. A függvénynek egy string paramétere legyen, az pedig az, hogy hova mentse el a csv-t (pl.: 'data/NJ.csv'). A függvény neve legyen 'save_first_60k'. \n",
    "    2.10 Írj egy függvényt ami a fenti függvényeket összefogja és megvalósítja (sorbarendezés --> drop_columns_and_nan --> ... --> save_first_60k), a függvény neve legyen 'prep_df'. Egy paramnétert várjon, az pedig a csv-nek a mentési útvonala legyen. Ha default value-ja legyen 'data/NJ.csv'\n",
    "\n",
    "3.  A feladatot a HAZI06.py-ban old meg.\n",
    "    Az órán megírt DecisionTreeClassifier-t fit-eld fel az első feladatban lementett csv-re. \n",
    "    A feladat célja az, hogy határozzuk meg azt, hogy a vonatok késnek-e vagy sem. 0p <= x < 5p --> nem késik (0), ha 5p <= x --> késik (1).\n",
    "    Az adatoknak a 20% legyen test és a splitelés random_state-je pedig 41 (mint órán)\n",
    "    A testset-en 80% kell elérni. Ha megvan a minimum százalék, akkor azzal paraméterezd fel a decisiontree-t és azt kell leadni.\n",
    "\n",
    "    A leadásnál csak egy fit kell, ezt azzal a paraméterre paraméterezd fel, amivel a legjobb accuracy-t elérted.\n",
    "\n",
    "    A helyes paraméter megtalálásához használhatsz grid_search-öt.\n",
    "    https://www.w3schools.com/python/python_ml_grid_search.asp \n",
    "\n",
    "4.  A tanításodat foglald össze 4-5 mondatban a HAZI06.py-ban a fájl legalján kommentben. Írd le a nehézségeket, mivel próbálkoztál, mi vált be és mi nem. Ezen kívül írd le 10 fitelésed eredményét is, hogy milyen paraméterekkel probáltad és milyen accuracy-t értél el. \n",
    "Ha ezt feladatot hiányzik, akkor nem fogadjuk el a házit!\n",
    "\n",
    "HAZI-\n",
    "    HAZI06-\n",
    "        -NJCleaner.py\n",
    "        -HAZI06.py\n",
    "\n",
    "##################################################################\n",
    "##                                                              ##\n",
    "## A feladatok közül csak a NJCleaner javítom unit test-el      ##\n",
    "## A decision tree-t majd manuálisan fogom lefuttatni           ##\n",
    "## NJCleaner - 10p, Tanítás - acc-nál 10%-ként egy pont         ##\n",
    "## Ha a 4. feladat hiányzik, akkor nem tudjuk elfogadni a házit ##\n",
    "##                                                              ##\n",
    "##################################################################\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8035833333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.DecisionTreeClassifier import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "col_name = ['stop_sequence', 'from_id', 'to_id', 'status', 'line','type','day','part_of_the_day', 'delay']\n",
    "data = pd.read_csv('data/NJ_60k.csv', skiprows=1, header=None, names=col_name)\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test  = train_test_split(X, Y, test_size=.2, random_state=41)\n",
    "classifier = DecisionTreeClassifier(min_samples_split=100, max_depth=11)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nÖsszefoglaló:\\nA sample és a depth a fa komplexitását befolyásolják ezért szélsőséges értékek esetén overfitting vagy underfitting keletkezhet,\\nami bár jó erdeményhez is vezethet, új adatok esetén nem fog jó eredményt produkálni a tree.\\nAz optimális beállítások során megpróbáltam egyesével végig próbálgatni a különböző bemeneti paramétereket, \\nabban reménykedve, hogy megtalálom az összefüggést a paraméterek között, de nem igazán jártam sikerrel sajnos.\\nA legeredményesebbnek az ígérkezett hogy a max depth-et 10 körüli számként tartom, a sample sizeot pedig kb tízszereseként a maxdepthnek. \\nA legjobb eredményt így sample=100 és depth=10-el értem el, mely 0.802 accuracyt ért el.\\n\\n\\nTest Results:\\nmin_samples_split=10, max_depth=6 : 0.7885\\nmin_samples_split=10, max_depth=10 : error\\nmin_samples_split=10, max_depth=11 : error\\nmin_samples_split=10, max_depth=30 : error\\nmin_samples_split=75, max_depth=10 : 0.80125\\nmin_samples_split=100, max_depth=10 : 0.802\\nmin_samples_split=100, max_depth=20 : 0.7974\\nmin_samples_split=100, max_depth=30 : 0.797\\nmin_samples_split=200, max_depth=20 : 0.798\\nmin_samples_split=20, max_depth=20 : error\\nmin_samples_split=20, max_depth=10 : error \\nmin_samples_split=40, max_depth=20 : 78.54\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Összefoglaló:\n",
    "A sample és a depth a fa komplexitását befolyásolják ezért szélsőséges értékek esetén overfitting vagy underfitting keletkezhet,\n",
    "ami bár jó erdeményhez is vezethet, új adatok esetén nem fog jó eredményt produkálni a tree.\n",
    "Az optimális beállítások során megpróbáltam egyesével végig próbálgatni a különböző bemeneti paramétereket, \n",
    "abban reménykedve, hogy megtalálom az összefüggést a paraméterek között, de nem igazán jártam sikerrel sajnos.\n",
    "A legeredményesebbnek az ígérkezett hogy a max depth-et 10 körüli számként tartom, a sample sizeot pedig kb tízszereseként a maxdepthnek. \n",
    "A legjobb eredményt így sample=100 és depth=11-el értem el, mely 0.8036 accuracyt ért el.\n",
    "\n",
    "\n",
    "Test Results:\n",
    "min_samples_split=10, max_depth=6 : 0.7885\n",
    "min_samples_split=10, max_depth=10 : error\n",
    "min_samples_split=10, max_depth=11 : error\n",
    "min_samples_split=10, max_depth=30 : error\n",
    "min_samples_split=75, max_depth=10 : 0.80125\n",
    "min_samples_split=100, max_depth=10 : 0.802\n",
    "min_samples_split=100, max_depth=11 : 0.8036\n",
    "min_samples_split=100, max_depth=20 : 0.7974\n",
    "min_samples_split=100, max_depth=30 : 0.797\n",
    "min_samples_split=200, max_depth=20 : 0.798\n",
    "min_samples_split=20, max_depth=20 : error\n",
    "min_samples_split=20, max_depth=10 : error \n",
    "min_samples_split=40, max_depth=20 : 78.54\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
